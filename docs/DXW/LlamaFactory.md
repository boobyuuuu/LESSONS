# ã€å¤§æ¨¡å‹å¾®è°ƒã€‘ä½¿ç”¨ LLaMA-Factory å¾®è°ƒ LLaMA3

ğŸ¥ è§†é¢‘æ•™ç¨‹
- [YouTube](https://youtu.be/Hpc4QQQuLWM)
- [Bilibili](https://www.bilibili.com/video/BV1uw4m1S7Cd/?vd_source=2acabf9b10c0b70274da02f31cf31368)

## 1. å®éªŒç¯å¢ƒ

### âš™ï¸ 1.1 æœºå™¨

**Ubuntu**  

ä½¿ç”¨é•œåƒï¼š [hiyouga/LLaMA-Factory/LLaMAFactoryV3](https://www.codewithgpu.com/i/hiyouga/LLaMA-Factory/LLaMAFactoryV3)

*é—®é¢˜ï¼šæ— æ³•ä½¿ç”¨åŠ é€Ÿå™¨å¾®è°ƒï¼Œç¯å¢ƒæœ‰ç‚¹ä¸å®Œå…¨ï¼Œä½†é—®é¢˜ä¸æ˜¯å¾ˆå¤§*

```
   - PyTorch 2.0
   - Python 3.10 (Ubuntu 22.04)
   - Cuda 12.1
   - RTX 4090 (24GB) * 1
   - CPU: 12 vCPU Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz
```

å»ºè®®é•œåƒï¼š[hiyouga/LLaMA-Factory/LLaMA-Factory](https://www.codewithgpu.com/i/hiyouga/LLaMA-Factory/LLaMA-Factory)

### ğŸ¦„ 1.2 åŸºåº§æ¨¡å‹

åŸºäºä¸­æ–‡æ•°æ®è®­ç»ƒè¿‡çš„ LLaMA3 8B æ¨¡å‹ï¼š  
[shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)

ï¼ˆå¯é€‰ï¼‰é…ç½® hf å›½å†…é•œåƒç«™ï¼š
```bash
pip install -U huggingface_hub
pip install huggingface-cli

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download shenzhi-wang/Llama3-8B-Chinese-Chat --local-dir /root/autodl-tmp/models/Llama3-8B-Chinese-Chat1
```

ä¸‹è½½æ—¶é—´æœ‰ç‚¹é•¿

## 2. LLaMA-Factory æ¡†æ¶

### âš™ï¸ 2.1 å®‰è£…

é•œåƒä¸‹ï¼Œä¸éœ€è¦å†é…ç½®ç¯å¢ƒ

### ğŸ“‚ 2.2 å‡†å¤‡è®­ç»ƒæ•°æ®

è®­ç»ƒæ•°æ®ï¼š

- [diy.json](https://github.com/boobyuuuu/LESSONS/blob/main/docs/DXW/diy.json)

- [identity.json](https://github.com/boobyuuuu/LESSONS/blob/main/docs/DXW/identity.json)

å°†è®­ç»ƒæ•°æ®æ”¾åœ¨ LLaMA-Factory/data/diy.json   LLaMA-Factory/data/identity.json  
å¹¶ä¸”ä¿®æ”¹æ•°æ®æ³¨å†Œæ–‡ä»¶ï¼šLLaMA-Factory/data/dataset_info.jsonã€‚ä¿®æ”¹ä¸ºå¦‚ä¸‹ï¼š[dataset_info.json](https://github.com/boobyuuuu/LESSONS/blob/main/docs/DXW/dataset_info.json)

```json
"diy": {
    "file_name": "diy.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  },
  "identity": {
    "file_name": "identity.json"
  },
```

### ğŸŒ 2.3 å¯åŠ¨ Web UI

```shell
cd LLaMA-Factory
llamafactory-cli webui
```

### ğŸ”§ 2.4 å¾®è°ƒæ¨¡å‹

1. **ä½¿ç”¨ Web UI è®­ç»ƒ**

2. **ä½¿ç”¨å‘½ä»¤è¡Œæ‰§è¡Œ**
   > é…ç½®æ–‡ä»¶ä½äºï¼š[cust/train_llama3_lora_sft.yaml](https://github.com/echonoshy/cgft-llm/tree/master/llama-factory/cust)

æ„å»º cust/train_llama3_lora_sft.yaml
```yaml
cutoff_len: 1024
dataset: fintech,identity
dataset_dir: data
do_train: true
finetuning_type: lora
flash_attn: auto
fp16: true
gradient_accumulation_steps: 8
learning_rate: 0.0002
logging_steps: 5
lora_alpha: 16
lora_dropout: 0
lora_rank: 8
lora_target: q_proj,v_proj
lr_scheduler_type: cosine
max_grad_norm: 1.0
max_samples: 1000
model_name_or_path: /root/autodl-tmp/models/Llama3-8B-Chinese-Chat
num_train_epochs: 10.0
optim: adamw_torch
output_dir: saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-05-25-20-27-47
packing: false
per_device_train_batch_size: 2
plot_loss: true
preprocessing_num_workers: 16
report_to: none
save_steps: 100
stage: sft
template: llama3
use_unsloth: true
warmup_steps: 0
```

å‘½ä»¤è¡Œæ‰§è¡Œï¼š
```shell
llamafactory-cli train cust/train_llama3_lora_sft.yaml
```

### ğŸ’¬ 2.5 å¯¹è¯

1. **ä½¿ç”¨ Web UI ç•Œé¢è¿›è¡Œå¯¹è¯**
    ```shell
    llamafactory-cli webchat cust/train_llama3_lora_sft.yaml
    ```

2. **ä½¿ç”¨ç»ˆç«¯è¿›è¡Œå¯¹è¯**
    ```shell
    llamafactory-cli chat cust/train_llama3_lora_sft.yaml
    ```

3. **ä½¿ç”¨ OpenAI API é£æ ¼è¿›è¡Œå¯¹è¯**
    ```shell
    # æŒ‡å®šå¤šå¡å’Œç«¯å£
    CUDA_VISIBLE_DEVICES=0,1 API_PORT=8000 
    llamafactory-cli api cust/train_llama3_lora_sft.yaml
    ```

### ğŸ› ï¸ 2.6 æ¨¡å‹åˆå¹¶

å°† base model ä¸è®­ç»ƒå¥½çš„ LoRA Adapter åˆå¹¶æˆä¸€ä¸ªæ–°çš„æ¨¡å‹ã€‚  
âš ï¸ ä¸è¦ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹æˆ– `quantization_bit` å‚æ•°æ¥åˆå¹¶ LoRA é€‚é…å™¨ã€‚
```shell
llamafactory-cli export cust/merge_llama3_lora_sft.yaml
```

> ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå°±ä¸éœ€è¦å†åŠ è½½ LoRA é€‚é…å™¨ã€‚

### ğŸ”¢ 2.7 æ¨¡å‹é‡åŒ–

æ¨¡å‹é‡åŒ–ï¼ˆModel Quantizationï¼‰æ˜¯ä¸€ç§å°†æ¨¡å‹çš„å‚æ•°å’Œè®¡ç®—ä»é«˜ç²¾åº¦ï¼ˆé€šå¸¸æ˜¯ 32 ä½æµ®ç‚¹æ•°ï¼ŒFP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆå¦‚ 16 ä½æµ®ç‚¹æ•°ï¼ŒFP16ï¼Œæˆ–è€… 8 ä½æ•´æ•°ï¼ŒINT8ï¼‰çš„è¿‡ç¨‹ã€‚

## 3 å®éªŒç»“æœ

æœ€è¿‘ä¸€æ¬¡å¾®è°ƒï¼š

### 3.1 è®­ç»ƒå‚æ•°

è®­ç»ƒè½®æ¬¡ï¼š5000

è®­ç»ƒæ—¶é—´ï¼š3h

```
cutoff_len: 4096
dataset: diy,identity
dataset_dir: data
ddp_timeout: 180000000
do_train: true
finetuning_type: lora
flash_attn: auto
fp16: true
gradient_accumulation_steps: 8
include_num_input_tokens_seen: true
learning_rate: 5.0e-05
logging_steps: 5
lora_alpha: 16
lora_dropout: 0
lora_rank: 8
lora_target: all
lr_scheduler_type: cosine
max_grad_norm: 1.0
max_samples: 10000
model_name_or_path: /root/autodl-tmp/model
num_train_epochs: 1000.0
optim: adamw_torch
output_dir: saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-07-09-02-41-29
packing: false
per_device_train_batch_size: 2
plot_loss: true
preprocessing_num_workers: 16
report_to: none
save_steps: 100
stage: sft
template: llama3
warmup_steps: 0
```

### 3.2 LOSSå‡½æ•°

![alt text](training_loss.png)

è®­ç»ƒåœ¨500æ¬¡ä»¥å†…è¾¾åˆ°æ”¶æ•›ï¼Œä¹‹åæ³¨æ„è®­ç»ƒæ¬¡æ•°

### 3.3 å¯¹è¯æµ‹è¯•

- identity

![alt text](image-2.png)

- example

```
ä»¥ä¸‹æ˜¯ä½ éœ€è¦åˆ†æçš„è´¦æˆ·èŠ‚ç‚¹çš„äº¤æ˜“æ•°æ®:[05-17 09:01] èŠ‚ç‚¹ 22 äº¤æ˜“ 12.35736465 [05-17 12:04] èŠ‚ç‚¹ 215 äº¤æ˜“ 0.06161094 [05-17 12:10] èŠ‚ç‚¹ 26 äº¤æ˜“ 5.634368 [05-18 04:32] èŠ‚ç‚¹ 71 äº¤æ˜“ 0.09947227 [05-18 15:51] èŠ‚ç‚¹ 237 äº¤æ˜“ 0.0 [05-18 21:11] èŠ‚ç‚¹ 164 äº¤æ˜“ 0.55 [05-19 22:10] èŠ‚ç‚¹ 131 äº¤æ˜“ 0.739 [05-19 23:18] èŠ‚ç‚¹ 48 äº¤æ˜“ 0.0 [05-20 00:55] èŠ‚ç‚¹ 148 äº¤æ˜“ 60.0 [05-22 05:17] èŠ‚ç‚¹ 34 äº¤æ˜“ 0.0 [05-22 18:54] èŠ‚ç‚¹ 52 äº¤æ˜“ 0.5 [05-25 03:17] èŠ‚ç‚¹ 13 äº¤æ˜“ 0.0 [05-25 16:49] èŠ‚ç‚¹ 114 äº¤æ˜“ 2.5 [05-27 03:45] èŠ‚ç‚¹ 100 äº¤æ˜“ 8.99958 [05-28 23:00] èŠ‚ç‚¹ 52 äº¤æ˜“ 0.15 [06-01 00:51] èŠ‚ç‚¹ 154 äº¤æ˜“ 0.2 [06-03 03:08] èŠ‚ç‚¹ 67 äº¤æ˜“ 0.17 [06-05 09:50] èŠ‚ç‚¹ 163 äº¤æ˜“ 0.99913534142808 [06-09 13:38] èŠ‚ç‚¹ 75 äº¤æ˜“ 0.0 [06-09 22:03] èŠ‚ç‚¹ 240 äº¤æ˜“ 0.0 [06-10 21:27] èŠ‚ç‚¹ 80 äº¤æ˜“ 0.5 [06-11 15:38] èŠ‚ç‚¹ 196 äº¤æ˜“ 0.05 [06-11 16:38] èŠ‚ç‚¹ 83 äº¤æ˜“ 2.88108474 [06-12 02:19] èŠ‚ç‚¹ 121 äº¤æ˜“ 0.0 [06-12 04:23] èŠ‚ç‚¹ 23 äº¤æ˜“ 0.042 [06-12 09:17] èŠ‚ç‚¹ 87 äº¤æ˜“ 14.579988 [06-12 23:02] èŠ‚ç‚¹ 109 äº¤æ˜“ 1.0 [06-13 18:49] èŠ‚ç‚¹ 239 äº¤æ˜“ 0.361534754160263 [06-14 05:24] èŠ‚ç‚¹ 10 äº¤æ˜“ 0.0 [06-14 05:41] èŠ‚ç‚¹ 210 äº¤æ˜“ 0.0 [06-16 04:05] èŠ‚ç‚¹ 13 äº¤æ˜“ 0.69202992 [06-16 05:40] èŠ‚ç‚¹ 33 äº¤æ˜“ 0.06530515317026835 [06-17 15:26] èŠ‚ç‚¹ 16 äº¤æ˜“ 0.005 [06-18 09:28] èŠ‚ç‚¹ 59 äº¤æ˜“ 0.5 [06-18 21:31] èŠ‚ç‚¹ 37 äº¤æ˜“ 1.0 [06-19 02:45] èŠ‚ç‚¹ 190 äº¤æ˜“ 0.002 [06-21 19:41] èŠ‚ç‚¹ 77 äº¤æ˜“ 5.0 [06-22 16:41] èŠ‚ç‚¹ 190 äº¤æ˜“ 0.01 [06-22 21:04] èŠ‚ç‚¹ 81 äº¤æ˜“ 5.0 [06-24 09:33] èŠ‚ç‚¹ 198 äº¤æ˜“ 9.6 [06-26 22:30] èŠ‚ç‚¹ 27 äº¤æ˜“ 0.0 [06-28 00:24] èŠ‚ç‚¹ 131 äº¤æ˜“ 0.0 [06-28 04:01] èŠ‚ç‚¹ 48 äº¤æ˜“ 1.0 [06-29 00:44] èŠ‚ç‚¹ 146 äº¤æ˜“ 5.0 [06-29 17:14] èŠ‚ç‚¹ 48 äº¤æ˜“ 0.0 [06-29 21:02] èŠ‚ç‚¹ 243 äº¤æ˜“ 3.0 [06-30 14:46] èŠ‚ç‚¹ 92 äº¤æ˜“ 5.6886497743838e-05 [06-30 19:05] èŠ‚ç‚¹ 75 äº¤æ˜“ 1.2 [07-01 17:56] èŠ‚ç‚¹ 150 äº¤æ˜“ 0.06 [07-02 00:14] èŠ‚ç‚¹ 51 äº¤æ˜“ 0.0 [07-02 00:51] èŠ‚ç‚¹ 228 äº¤æ˜“ 2.0 [07-02 06:04] èŠ‚ç‚¹ 94 äº¤æ˜“ 0.0 [07-02 09:29] èŠ‚ç‚¹ 9 äº¤æ˜“ 0.0 [07-02 15:30] èŠ‚ç‚¹ 53 äº¤æ˜“ 0.08 [07-06 02:31] èŠ‚ç‚¹ 37 äº¤æ˜“ 0.0 [07-06 22:25] èŠ‚ç‚¹ 154 äº¤æ˜“ 0.2 [07-07 22:48] èŠ‚ç‚¹ 44 äº¤æ˜“ 0.0 [07-10 14:13] èŠ‚ç‚¹ 167 äº¤æ˜“ 0.0 [07-11 15:44] èŠ‚ç‚¹ 117 äº¤æ˜“ 0.0 [07-11 23:13] èŠ‚ç‚¹ 233 äº¤æ˜“ 0.0 [07-12 02:53] èŠ‚ç‚¹ 135 äº¤æ˜“ 2.0 [07-12 21:01] èŠ‚ç‚¹ 64 äº¤æ˜“ 9.17 [07-12 22:19] èŠ‚ç‚¹ 242 äº¤æ˜“ 2.43734798

è¿™æ˜¯å½“å‰èŠ‚ç‚¹å’Œå…¶ä»–èŠ‚ç‚¹çš„äº¤æ˜“æ•°æ®ï¼Œè¯·å¯¹è¿™ä¸ªèŠ‚ç‚¹ä½œå‡ºåˆ¤æ–­ï¼Œè¯ˆéª—èŠ‚ç‚¹è¾“å‡º1ï¼Œæ­£å¸¸èŠ‚ç‚¹è¾“å‡º0.
```

![alt text](image-3.png)

